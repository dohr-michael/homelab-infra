##############################################################################
# LobeChat — Interface chat LLM
#
# Se connecte à :
#   - llama-server (port 8080) pour le chat LLM via API OpenAI
#   - ComfyUI accessible directement via sd-api.home.dohrm.fr
#
# Mode client (sans base de données) — léger et simple
# Port : 3210
##############################################################################

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui
  namespace: ai-stack
  labels:
    app: open-webui
    component: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: open-webui
  template:
    metadata:
      labels:
        app: open-webui
        component: frontend
    spec:
      # ── Scheduling sur le noeud Strix Halo ────────────────
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "ai"
          effect: "NoSchedule"
      nodeSelector:
        gpu-type: "strix-halo"

      containers:
        - name: lobechat
          image: lobehub/lobe-chat:latest
          imagePullPolicy: IfNotPresent

          env:
            # ── Connexion au LLM (llama-server, API OpenAI-compatible) ──
            - name: OPENAI_API_KEY
              value: "sk-no-key-required"
            - name: OPENAI_PROXY_URL
              value: "http://llama-server.ai-stack.svc.cluster.local:8080/v1"
            - name: OPENAI_MODEL_LIST
              value: "qwen3-30b-a3b"

            # ── Désactiver Ollama ──
            - name: ENABLED_OLLAMA
              value: "0"

            # ── Accès ──
            - name: ACCESS_CODE
              valueFrom:
                secretKeyRef:
                  name: open-webui-secrets
                  key: WEBUI_SECRET_KEY

          ports:
            - name: http
              containerPort: 3210
              protocol: TCP

          startupProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 12

          readinessProbe:
            httpGet:
              path: /
              port: http
            periodSeconds: 15

          livenessProbe:
            httpGet:
              path: /
              port: http
            periodSeconds: 30
            failureThreshold: 3

          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "1"
              memory: "512Mi"

---
apiVersion: v1
kind: Service
metadata:
  name: open-webui
  namespace: ai-stack
  labels:
    app: open-webui
spec:
  type: ClusterIP
  selector:
    app: open-webui
  ports:
    - name: http
      port: 3210
      targetPort: http
      protocol: TCP
