##############################################################################
# llama-server (ROCm) — Qwen3-Coder-30B-A3B
#
# Image : kyuz0/amd-strix-halo-toolboxes:rocm-6.4.4
# Backend : ROCm 6.4.4 (AMD native GPU compute)
# Modèle : Qwen3-Coder-30B-A3B-Instruct (MoE, 3.3B params actifs)
#           → Spécialisé coding + agentic tool calling
#           → ~18.6 Go en Q4_K_M, contexte natif 256K
#
# API compatible OpenAI sur le port 8080
##############################################################################

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-server-config
  namespace: ai-stack
data:
  # ── Modèle ──────────────────────────────────────────────────
  # Ajuster selon le modèle téléchargé
  # MODEL_PATH: "/models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
  # MODEL_PATH: "/models/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf"
  MODEL_PATH: "/models/Qwen3-30B-A3B-Q4_K_M.gguf"
  
  # ── Paramètres serveur ──────────────────────────────────────
  HOST: "0.0.0.0"
  PORT: "8080"

  # ── Paramètres inférence ────────────────────────────────────
  # -ngl 999 : offload tous les layers sur GPU
  # -fa      : flash attention (OBLIGATOIRE sur Strix Halo)
  # -c       : taille du contexte
  # -np      : nombre de slots parallèles
  # --no-mmap: évite les crashs mmap sur gfx1151
  N_GPU_LAYERS: "999"
  CTX_SIZE: "65536"
  PARALLEL_SLOTS: "2"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  namespace: ai-stack
  labels:
    app: llama-server
    component: llm
spec:
  replicas: 1
  strategy:
    type: Recreate           # GPU partagé, pas de rolling update
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
        component: llm
    spec:
      # ── Scheduling sur le noeud Strix Halo ────────────────
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "ai"
          effect: "NoSchedule"
      nodeSelector:
        gpu-type: "strix-halo"

      # ── Validation des modèles ──────────────────────────────
      initContainers:
        - name: check-model
          image: busybox:1.37
          command: ["sh", "-c"]
          args:
            - |
              echo "Checking model file: $(MODEL_PATH)"
              if [ ! -f "$(MODEL_PATH)" ]; then
                echo "ERROR: Model file not found: $(MODEL_PATH)"
                echo "Available files in /models:"
                ls -lh /models/ 2>/dev/null || echo "  (directory empty or missing)"
                exit 1
              fi
              echo "OK: Model file found ($(du -h "$(MODEL_PATH)" | cut -f1))"
          envFrom:
            - configMapRef:
                name: llama-server-config
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true

      # ── Conteneur principal ────────────────────────────────
      containers:
        - name: llama-server
          image: docker.io/kyuz0/amd-strix-halo-toolboxes:rocm-6.4.4
          imagePullPolicy: Always

          command: ["llama-server"]
          args:
            - "-m"
            - "$(MODEL_PATH)"
            - "--host"
            - "$(HOST)"
            - "--port"
            - "$(PORT)"
            - "-ngl"
            - "$(N_GPU_LAYERS)"
            - "-fa"
            - "1"
            - "-c"
            - "$(CTX_SIZE)"
            - "-np"
            - "$(PARALLEL_SLOTS)"
            - "--no-mmap"
            - "-fit"
            - "off"
            # New one
            - "--cache-type-k"
            - "q8_0"
            - "--cache-type-v" 
            - "q8_0"
            - "--cont-batching"

          envFrom:
            - configMapRef:
                name: llama-server-config

          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          # ── Accès GPU ROCm ─────────────────────────────────
          # /dev/dri + /dev/kfd requis par ROCm
          volumeMounts:
            - name: dri
              mountPath: /dev/dri
            - name: kfd
              mountPath: /dev/kfd
            - name: models
              mountPath: /models
              readOnly: true

          # ── Santé ──────────────────────────────────────────
          startupProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 30     # Jusqu'à 5 min pour charger le modèle
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /health
              port: http
            periodSeconds: 15
            timeoutSeconds: 5

          livenessProbe:
            httpGet:
              path: /health
              port: http
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3

          # ── Ressources ─────────────────────────────────────
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "8"
              memory: "16Gi"

          # ── Sécurité ───────────────────────────────────────
          # privileged requis : SELinux bloque les allocations mémoire HSA de ROCm
          securityContext:
            privileged: true
            runAsUser: 0

      # ── Volumes ──────────────────────────────────────────
      volumes:
        - name: dri
          hostPath:
            path: /dev/dri
            type: Directory
        - name: kfd
          hostPath:
            path: /dev/kfd
            type: CharDevice
        - name: models
          persistentVolumeClaim:
            claimName: ai-models-llm-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: llama-server
  namespace: ai-stack
  labels:
    app: llama-server
spec:
  type: ClusterIP
  selector:
    app: llama-server
  ports:
    - name: http
      port: 8080
      targetPort: http
      protocol: TCP
