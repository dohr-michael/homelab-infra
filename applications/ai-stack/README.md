# ğŸš€ Stack AI sur K3S â€” AMD Strix Halo (Vulkan)

Stack complÃ¨te pour exÃ©cuter des LLMs et de la gÃ©nÃ©ration d'images sur un noeud AMD Strix Halo (gfx1151) via **Vulkan**, dÃ©ployÃ©e sur K3S.

## Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Cluster K3S (4 noeuds)              â”‚
                    â”‚                                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚          â”‚       â”‚  â”‚  Noeud Strix Halo (96 Go RAM partagÃ©e)  â”‚ â”‚
â”‚  Client  â”‚â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”‚                                         â”‚ â”‚
â”‚ navigateur       â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚          â”‚       â”‚  â”‚  â”‚ llama-server  â”‚  â”‚  sd-cpp-vulkan  â”‚  â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚  â”‚ (Qwen3 30B)  â”‚  â”‚  (SDXL Turbo)   â”‚  â”‚ â”‚
                   â”‚  â”‚  â”‚ Vulkan :8080  â”‚  â”‚  Vulkan  :7860  â”‚  â”‚ â”‚
                   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
                   â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ â”‚
                   â”‚  â”‚           /dev/dri (iGPU Vulkan)        â”‚ â”‚
                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                   â”‚                                             â”‚
                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                   â”‚  â”‚  Open WebUI (tout noeud)  â”‚               â”‚
                   â”‚  â”‚  :3000                    â”‚               â”‚
                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## PrÃ©requis

### Sur le noeud Strix Halo

#### 1. Kernel â‰¥ 6.18.4
```bash
uname -r
# Si < 6.18.4, mettre Ã  jour â€” les kernels plus anciens ont un bug gfx1151
```

#### 2. Firmware
```bash
# âš ï¸ NE PAS utiliser linux-firmware-20251125 (casse ROCm/Vulkan)
# Utiliser â‰¥ 20260110 ou une version antÃ©rieure stable
rpm -q linux-firmware
```

#### 3. VÃ©rifier Vulkan
```bash
vulkaninfo --summary
# Doit afficher : AMD Radeon Graphics (RADV GFX1151)
```

#### 4. Labels et taints du noeud
```bash
# Label pour le nodeSelector
kubectl label node <NOM_NOEUD_STRIX> gpu-type=strix-halo

# Taint pour rÃ©server le noeud aux workloads AI
kubectl taint node <NOM_NOEUD_STRIX> dedicated=ai:NoSchedule
```

#### 5. BIOS â€” Allocation mÃ©moire GPU
Dans le BIOS de la GMKTech, allouer le maximum de mÃ©moire au GPU (UMA).
IdÃ©alement : **~80-88 Go GPU / ~8-16 Go systÃ¨me**.

#### 6. CrÃ©er les rÃ©pertoires de modÃ¨les
```bash
sudo mkdir -p /srv/ai-models/{llm,diffusion}
sudo chown -R 1000:1000 /srv/ai-models
```

### TÃ©lÃ©charger les modÃ¨les

#### LLM â€” Qwen3-30B-A3B-Instruct (MoE)
```bash
# Option A : Q4_K_M (~10 Go) â€” bon compromis qualitÃ©/taille
# Laisse beaucoup de place pour la gÃ©nÃ©ration d'images
curl -L -o /srv/ai-models/llm/Qwen3-30B-A3B-Instruct-Q4_K_M.gguf \
  "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-GGUF/resolve/main/qwen3-30b-a3b-instruct-q4_k_m.gguf"

# Option B : BF16 (~17 Go) â€” qualitÃ© maximale, plus lent
# huggingface-cli download unsloth/Qwen3-30B-A3B-Instruct-GGUF \
#   BF16/Qwen3-30B-A3B-Instruct-BF16-00001-of-00002.gguf \
#   --local-dir /srv/ai-models/llm/

# Option C : Qwen3-32B dense Q4_K_M (~20 Go) â€” si vous prÃ©fÃ©rez un modÃ¨le dense
# curl -L -o /srv/ai-models/llm/Qwen3-32B-Q4_K_M.gguf \
#   "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/qwen3-32b-q4_k_m.gguf"
```

#### Image Gen â€” SDXL Turbo
```bash
curl -L -o /srv/ai-models/diffusion/sd_xl_turbo_1.0_fp16.safetensors \
  "https://huggingface.co/stabilityai/sdxl-turbo/resolve/main/sd_xl_turbo_1.0_fp16.safetensors"
```

## DÃ©ploiement

### Ã‰tape 1 â€” Builder l'image stable-diffusion.cpp
```bash
# Sur une machine avec Docker (peut Ãªtre le noeud Strix Halo)
cd docker/
docker build -t sd-cpp-vulkan:latest -f Dockerfile.sd-cpp .

# Si vous avez un registry privÃ© :
docker tag sd-cpp-vulkan:latest <votre-registry>/sd-cpp-vulkan:latest
docker push <votre-registry>/sd-cpp-vulkan:latest
# â†’ Puis modifier l'image dans 20-sd-server.yaml

# Si pas de registry, importer directement dans K3S (sur le noeud) :
docker save sd-cpp-vulkan:latest | sudo k3s ctr images import -
```

### Ã‰tape 2 â€” Appliquer les manifestes
```bash
# Tout d'un coup
kubectl apply -f 00-namespace.yaml
kubectl apply -f 01-storage.yaml
kubectl apply -f 10-llama-server.yaml
kubectl apply -f 20-sd-server.yaml
kubectl apply -f 30-open-webui.yaml

# Optionnel : Ingress pour accÃ¨s externe
kubectl apply -f 40-ingress.yaml
```

### Ã‰tape 3 â€” VÃ©rifier le dÃ©ploiement
```bash
# Statut des pods
kubectl -n ai-stack get pods -w

# Logs llama-server (vÃ©rifier que Vulkan est dÃ©tectÃ©)
kubectl -n ai-stack logs -f deployment/llama-server
# Chercher : "ggml_vulkan: Found 1 Vulkan devices"
# Chercher : "AMD Radeon Graphics (RADV GFX1151)"

# Logs sd-server
kubectl -n ai-stack logs -f deployment/sd-server

# Logs Open WebUI
kubectl -n ai-stack logs -f deployment/open-webui
```

### Ã‰tape 4 â€” AccÃ©der Ã  l'interface
```bash
# Port-forward rapide (sans Ingress)
kubectl -n ai-stack port-forward svc/open-webui 3000:3000

# Ouvrir http://localhost:3000
# CrÃ©er un compte admin au premier accÃ¨s
```

## Configuration Open WebUI

AprÃ¨s le premier login dans Open WebUI :

1. **LLM** : Aller dans `Settings â†’ Connections`
   - L'URL OpenAI devrait dÃ©jÃ  Ãªtre configurÃ©e via les variables d'env
   - VÃ©rifier que les modÃ¨les Qwen3 apparaissent

2. **Image Generation** : Aller dans `Settings â†’ Images`
   - Engine : `AUTOMATIC1111`
   - URL : `http://sd-server.ai-stack.svc.cluster.local:7860`
   - Activer "Image Generation"
   - Configurer la rÃ©solution (512x512 pour SDXL Turbo)

## Test rapide des APIs

```bash
# â”€â”€ Test LLM (depuis un pod du cluster) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
curl http://llama-server.ai-stack.svc.cluster.local:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [{"role": "user", "content": "Bonjour, qui es-tu ?"}],
    "max_tokens": 100
  }'

# â”€â”€ Test Image Gen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
curl http://sd-server.ai-stack.svc.cluster.local:7860/sdapi/v1/txt2img \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "a beautiful sunset over mountains",
    "steps": 4,
    "width": 512,
    "height": 512
  }'
```

## Budget mÃ©moire (96 Go partagÃ©e)

| Composant                    | RAM GPU estimÃ©e |
|------------------------------|----------------|
| Qwen3-30B-A3B Q4_K_M        | ~10-12 Go      |
| KV Cache (ctx 8192, 2 slots) | ~2-4 Go        |
| SDXL Turbo FP16              | ~6.5 Go        |
| Overhead systÃ¨me + Vulkan    | ~4-6 Go        |
| **Total estimÃ©**             | **~25-30 Go**  |
| **Disponible restant**       | **~60-65 Go**  |

> Vous avez largement de la marge ! Vous pourriez monter en Qwen3-32B dense
> ou utiliser un modÃ¨le de diffusion plus gros (Flux.1, etc.)

## Troubleshooting

### Le pod llama-server ne dÃ©marre pas
```bash
# VÃ©rifier que /dev/dri est accessible
kubectl -n ai-stack exec -it deployment/llama-server -- ls -la /dev/dri/

# VÃ©rifier Vulkan dans le conteneur
kubectl -n ai-stack exec -it deployment/llama-server -- vulkaninfo --summary
```

### Crash avec "mmap" errors
VÃ©rifier que `--no-mmap` est bien passÃ© en argument. C'est **obligatoire** sur gfx1151.

### Performances faibles
- VÃ©rifier que `-fa` (flash attention) est activÃ©
- VÃ©rifier l'allocation mÃ©moire GPU dans le BIOS
- VÃ©rifier la version du kernel (â‰¥ 6.18.4)
- Monitorer avec `amdgpu_top` sur le noeud hÃ´te

### Open WebUI ne voit pas les modÃ¨les
```bash
# Tester l'API directement
kubectl -n ai-stack exec -it deployment/open-webui -- \
  curl http://llama-server:8080/v1/models
```

## Fichiers

```
k3s-ai-stack/
â”œâ”€â”€ 00-namespace.yaml        # Namespace ai-stack
â”œâ”€â”€ 01-storage.yaml          # PV/PVC pour les modÃ¨les
â”œâ”€â”€ 10-llama-server.yaml     # LLM : llama.cpp Vulkan + Qwen3
â”œâ”€â”€ 20-sd-server.yaml        # Image Gen : sd.cpp Vulkan
â”œâ”€â”€ 30-open-webui.yaml       # Interface Web unifiÃ©e
â”œâ”€â”€ 40-ingress.yaml          # Ingress (optionnel)
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile.sd-cpp    # Build de l'image sd.cpp Vulkan
â””â”€â”€ README.md                # Ce fichier
```
